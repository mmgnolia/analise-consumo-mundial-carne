# ü•© Consumo Mundial de Carne ‚Äî Dashboard Interativo

Este reposit√≥rio cont√©m um projeto completo de **Data Analytics** desenvolvido para demonstrar uma **pipeline de ETL + Visualiza√ß√£o de Dados**, utilizando **PySpark**, **MongoDB Atlas** e **Streamlit**.

O dashboard final permite analisar o consumo global de carne **Bovina, Su√≠na, Aves e Ovinos**, com filtros por pa√≠s, per√≠odo e tipo de m√©trica (KG per Capita ou Milhares de Toneladas).


## Arquitetura e Fluxo de Dados

A solu√ß√£o segue uma arquitetura simples e robusta dividida em tr√™s etapas:

1. **Extra√ß√£o & Transforma√ß√£o (ETL):**
   Processamento do CSV bruto com Python e PySpark.

2. **Armazenamento (MongoDB ‚Äì NoSQL):**
   Os dados transformados s√£o enviados para o **MongoDB Atlas**, estruturados em formato aninhado e otimizado para consultas r√°pidas.

3. **Visualiza√ß√£o (Streamlit):**
   O dashboard consome diretamente os dados do cluster MongoDB para gerar mapas, gr√°ficos e KPIs em tempo real.

## Fonte dos Dados

Os dados brutos foram obtidos no Kaggle:

**Dataset:** *OECD - Meat Consumption Dataset*
**Arquivo original:** `oecd-meat-consumption.csv`
Link: [https://www.kaggle.com/datasets/sariag/consumption-of-meat-worldwide](https://www.kaggle.com/datasets/sariag/consumption-of-meat-worldwide)

O arquivo est√° localizado na pasta:

```
data_raw/
```


## Componentes do Reposit√≥rio

| Arquivo/Pasta            | Fun√ß√£o                                                             | Tecnologias                |
| ------------------------ | ------------------------------------------------------------------ | -------------------------- |
| `data_raw/`              | Armazena o CSV original vindo do Kaggle.                           | -                          |
| `data_process.py`        | Limpa, transforma e organiza os dados (join + pivot).              | PySpark, Pandas            |
| `load_mongo.py`          | Estrutura os dados de forma aninhada e envia para o MongoDB Atlas. | PyMongo, Pandas            |
| `app_com_comentarios.py` | Dashboard interativo com gr√°ficos, KPIs e filtros din√¢micos.       | Streamlit, Plotly, PyMongo |
| `requirements.txt`       | Depend√™ncias do ambiente.                                          | -                          |
| `.gitignore`             | Garante que arquivos sens√≠veis e de cache n√£o sejam versionados.   | -                          |



## ‚öôÔ∏è Pr√©-requisitos e Configura√ß√£o

### 1. Requisitos

* Python **3.8+**
* PySpark instalado
* Conta no **MongoDB Atlas**
* Connection String do seu cluster



### 2. Configura√ß√£o do Ambiente

```bash
# Criar ambiente virtual
python -m venv venv

# Ativar (Windows)
.\venv\Scripts\activate

# Ativar (Linux/macOS)
source venv/bin/activate

# Instalar depend√™ncias
pip install -r requirements.txt
```


### 3. Configura√ß√£o do MongoDB Atlas 

√â indispens√°vel a conex√£o no MongoDB Atlas, copie e cole em app.py a sua conex√£o de string:

#### Substitua no c√≥digo:

```cmd
connection_string = "mongodb+srv://user:<password>..."
```


## Execu√ß√£o do Pipeline ETL

### 1. Processamento dos Dados (PySpark)

```bash
python data_process.py
```

Gera o arquivo:

```
data_processed/consumo_processado.parquet
```

### 2. Carga no MongoDB (PyMongo)

```bash
python load_mongo.py
```

Insere os dados na cole√ß√£o:

```
dados_processados
```


## Iniciar o Dashboard

Com os dados no MongoDB, execute:

```bash
python -m streamlit run dashboard/app.py  
```

O dashboard ser√° aberto no navegador (porta padr√£o: `8501`).


## üìä An√°lises Dispon√≠veis

* **Mapa Mundial (Choropleth):**
  M√©dia do consumo por pa√≠s, com op√ß√£o para visualizar KG per Capita ou Mil Toneladas.

* **Evolu√ß√£o Temporal:**
  Gr√°ficos de linha mostrando a tend√™ncia hist√≥rica por pa√≠s.

* **Comparativo por Dieta:**
  Exibe a propor√ß√£o de cada tipo de carne consumida.

* **Relat√≥rio Executivo:**
  KPIs como maior consumidor per capita e varia√ß√£o percentual ao longo do per√≠odo.

